from pyspark.sql import SparkSession
from pyspark.sql.functions import from_json, col
from pyspark.sql.types import StructType, StructField, StringType, DoubleType
from kafka import KafkaConsumer
import json

# Initialize SparkSession
spark = SparkSession.builder \
    .appName("WeatherDataStreaming") \
    .getOrCreate()

# Define schema for weather data
weather_schema = StructType([
    StructField("Location", StringType(), True),
    StructField("Date", StringType(), True),
    StructField("Min Temperature", DoubleType(), True),
    StructField("Max Temperature", DoubleType(), True),
    StructField("Precipitation", DoubleType(), True),
    StructField("Wind Speed", DoubleType(), True),
    StructField("Humidity", DoubleType(), True)
])

# Kafka consumer configuration
consumer = KafkaConsumer('weather_data',
                         bootstrap_servers='localhost:9092',
                         auto_offset_reset='earliest',  # Start consuming from beginning of topic
                         group_id='weather-group',  # Consumer group ID
                         value_deserializer=lambda m: json.loads(m.decode('utf-8')))

# Consume messages from Kafka
for message in consumer:
    weather_data = message.value

    # Convert JSON data to Spark DataFrame with schema
    # Handle type conversion from string to appropriate types
    weather_data_converted = {
        "Location": weather_data["Location"],
        "Date": weather_data["Date"],
        "Min Temperature": float(weather_data["Min Temperature"]),
        "Max Temperature": float(weather_data["Max Temperature"]),
        "Precipitation": float(weather_data["Precipitation"]),
        "Wind Speed": float(weather_data["Wind Speed"]),
        "Humidity": float(weather_data["Humidity"])
    }

    weather_df = spark.createDataFrame([weather_data_converted], schema=weather_schema)

    # Perform further processing or analysis on the data
    weather_df.show()

# Close the consumer when done
consumer.close()

# Stop the SparkSession
spark.stop()
